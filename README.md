# Text-to-Image Generation Using CLIP and Stable Diffusion

This repository contains the implementation and evaluation of a complete text-to-image generation pipeline. The system integrates CLIP text encoding with Stable Diffusion v1.5 for semantic image synthesis.

---

##  Repository Structure
```
milestone1/          ‚Üí Dataset preparation & baseline generation
‚îú‚îÄ‚îÄ download_dataset.py       ‚Üí Download COCO 2017 dataset
‚îú‚îÄ‚îÄ explore_dataset.py        ‚Üí Visualize dataset samples
‚îú‚îÄ‚îÄ test_clip.py              ‚Üí Validate CLIP text encoding
‚îî‚îÄ‚îÄ generate_images.py        ‚Üí Generate 5 baseline images

milestone2/          ‚Üí Parameter optimization experiments
‚îú‚îÄ‚îÄ experiment_cfg.py         ‚Üí Test CFG scales (3.0-15.0)
‚îú‚îÄ‚îÄ experiment_steps.py       ‚Üí Test inference steps (10-50)
‚îú‚îÄ‚îÄ experiment_schedulers.py  ‚Üí Test schedulers (PNDM, DDIM, LMS, Euler)
‚îú‚îÄ‚îÄ generate_final_set.py     ‚Üí Generate 10 optimized images
‚îú‚îÄ‚îÄ training_log.md           ‚Üí Detailed experiment observations
‚îî‚îÄ‚îÄ milestone2_summary.md     ‚Üí 1-page findings summary

milestone3/          ‚Üí Quantitative evaluation & metrics
‚îú‚îÄ‚îÄ prepare_reference_images.py    ‚Üí Sample 500 COCO images
‚îú‚îÄ‚îÄ resize_reference_images.py     ‚Üí Resize to 512√ó512
‚îú‚îÄ‚îÄ calculate_fid.py               ‚Üí FID score calculation
‚îú‚îÄ‚îÄ calculate_inception_score.py   ‚Üí Inception Score calculation
‚îú‚îÄ‚îÄ calculate_clip_similarity.py   ‚Üí CLIP similarity per image
‚îú‚îÄ‚îÄ parameter_analysis.py          ‚Üí Create 4-panel chart
‚îú‚îÄ‚îÄ visualize_comparison.py        ‚Üí Create comparison charts
‚îî‚îÄ‚îÄ milestone3_results.md          ‚Üí Comprehensive results

demo/                ‚Üí Interactive web application
‚îî‚îÄ‚îÄ app.py           ‚Üí Streamlit interface with 6 advanced features

datasets/            ‚Üí COCO 2017 validation (gitignored, 1.25GB)
```

---

##  Milestones Summary

### Milestone 1 ‚Äì Dataset Preparation & Baseline

**Objective:** Establish functional pipeline with CLIP + Stable Diffusion

**Tasks:**
- Downloaded COCO 2017 validation set (5,000 images, 25,014 captions)
- Validated CLIP text encoding (83.24% matching accuracy)
- Generated 5 baseline images with default parameters

**Key Finding:** System functional but shows style inconsistencies

---

### Milestone 2 ‚Äì Parameter Optimization

**Objective:** Identify optimal generation parameters through systematic experiments

**Experiments Conducted:**

**1. CFG Scale (5 values tested)**
- Optimal: **CFG 7.5** (31s)
- CFG 15.0: 222s (7√ó slower, minimal gain)

**2. Inference Steps (4 values tested)**
- Optimal: **20 steps** (30s)
- 50 steps: 139s (4.6√ó slower, marginal improvement)

**3. Noise Schedulers (4 tested)**
- PNDM: 32s (default, reliable)
- DDIM: 30s (fastest, deterministic)

**Final Generation:** 10 diverse images using optimal settings (CFG=7.5, 20 steps)

**Key Finding:** Small parameter changes = massive performance differences (7√ó speed variation)

---

### Milestone 3 ‚Äì Quantitative Evaluation

**Objective:** Measure performance with industry-standard metrics

**Metrics Calculated:**

**FID (Fr√©chet Inception Distance):** 374.47
- Generated: 10 images
- Reference: 500 COCO images (resized 512√ó512)
- Interpretation: Photorealism limited (baseline model, no fine-tuning)

**Inception Score:** 5.08
- Threshold: >5.0 = Excellent
- Interpretation: Clear, diverse outputs 

**CLIP Similarity:** 31.85 (range: 28.66-34.16)
- Threshold: >0.30 = Excellent
- Interpretation: Perfect semantic alignment 

**Visualizations:** 4 comprehensive charts created

**Key Finding:** Strong understanding + diversity, weak photorealism ‚Üí baseline model limitation

---

## üñ•Ô∏è How to Run

### 1. Setup Environment
```bash
cd Text-to-Image-Generation
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Download Dataset (First Time Only)
```bash
cd milestone1
python download_dataset.py
# Takes 10-20 minutes, downloads 1.25GB
```

### 3. Run Baseline Generation
```bash
# Test CLIP
python test_clip.py

# Generate 5 baseline images
python generate_images.py
# Output: milestone1/generated_images/ (5 images, ~3 min)
```

### 4. Run Optimization Experiments
```bash
cd ../milestone2

# CFG experiment (5 images)
python experiment_cfg.py

# Steps experiment (4 images)
python experiment_steps.py

# Scheduler experiment (4 images)
python experiment_schedulers.py

# Generate final optimized set (10 images)
python generate_final_set.py
```

### 5. Calculate Metrics
```bash
cd ../milestone3

# Prepare reference data
python prepare_reference_images.py
python resize_reference_images.py

# Calculate all metrics
python calculate_fid.py          # FID: 374.47
python calculate_inception_score.py  # IS: 5.08
python calculate_clip_similarity.py  # CLIP: 31.85

# Create visualizations
python parameter_analysis.py
python visualize_comparison.py
```

### 6. Run Interactive Demo
```bash
cd ../demo
streamlit run app.py
# Opens browser at localhost:8501
```

---

##  Demo Features (Beyond Requirements)

1. **Spell Correction** - Auto-fixes typos (TextBlob)
2. **Style Presets** - 7 artistic styles (Photorealistic, Anime, Oil Painting, etc.)
3. **Negative Prompts** - Specify unwanted elements
4. **Quality Scoring** - Real-time CLIP similarity feedback
5. **Batch Generation** - Create 1-4 variations
6. **Prompt History** - Reuse previous prompts

---

##  Results Summary

### Parameter Optimization

| Parameter | Tested Values | Optimal | Speedup |
|-----------|--------------|---------|---------|
| CFG Scale | 3.0-15.0 | 7.5 | 7√ó vs CFG 15 |
| Inference Steps | 10-50 | 20 | 4.6√ó vs 50 steps |
| Scheduler | 4 types | PNDM/DDIM | Minimal impact |

### Performance Metrics

- **Generation Time:** 35-40s avg (Apple Silicon MPS)
- **Resolution:** 512√ó512
- **Hardware:** Mac (MPS acceleration)
- **Sample Size:** 10 final images, 23 experimental images

### Evaluation Scores

- **FID:** 374.47 (needs photorealism improvement)
- **IS:** 5.08 (excellent clarity + diversity)
- **CLIP:** 31.85 (perfect semantic alignment)

---

##  Known Limitations

- **Style inconsistency** - Some images render in cartoon/illustration style
- **Small sample** - 10 images insufficient for robust FID
- **Resolution** - Limited to 512√ó512 on consumer hardware
- **Speed** - 35s on Mac vs 5-10s on A100 GPU

**Root Cause:** Baseline Stable Diffusion trained on mixed data (photos + art + cartoons) without fine-tuning

---

## Future Improvements

- Fine-tune on COCO for photorealism (expect FID: 100-150)
- Generate 100+ images for robust metrics
- Higher resolution (768√ó768, 1024√ó1024)
- ControlNet for spatial control
- Faster generation (<10s)

---

##  Team 

- **Harini Prasad Vasisht** - vasisht.h@northeastern.edu
- **Samruddhi Bansod** - bansod.s@northeastern.edu
- **Pranav Rangbulla** - rangbulla.p@northeastern.edu
- **Dhanush Manoharan** - manoharan.d@northeastern.edu

---

## üìÑ Citation
```bibtex
@project{vasisht2025texttoimage,
  title={Text-to-Image Generation Using CLIP and Stable Diffusion},
  author={Vasisht, Harini Prasad and Bansod, Samruddhi and Rangbulla, Pranav and Manoharan, Dhanush},
  institution={Northeastern University},
  course={IE 7615 Deep Learning for AI},
  year={2025}
}
```

---

##  References

[1] Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models" (2022)  
[2] Radford et al. "Learning Transferable Visual Models From Natural Language Supervision" (2021)  
[3] Schuhmann et al. "LAION-5B: An Open Large-Scale Dataset" (2022)  
[4] Lin et al. "Microsoft COCO: Common Objects in Context" (2014)

---

